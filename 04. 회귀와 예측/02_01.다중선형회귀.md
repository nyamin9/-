# 🎰 02_01. 다중선형회귀  

🎲 하나의 종속변수와 여러 독립변수 간의 관계를 나타냄  


🎲 독립변수가 여러개인 경우 수식은 이들을 모두 포함하는 다음과 같은 형태가 됨  

$Y = b_0 + b_1X_1 + b_2X_2 + ... + b_pX_p + e$  

<br>  


🎲 이는 직선의 형태는 아니지만, 각 계수와 그 변수들 사이의 관계는 선형이므로 선형모형으로 취급함  

<br>  

🎲 용어 정리  

- 제곱근평균제곱오차 (RMSE, root mean squared error) : 회귀 시 평균제곱오차의 제곱근. 회귀모형 평가에 가장 널리 사용되는 측정 지표  
   
- 잔차 표준오차 (RSE, residual standard error) : 평균제곱오차와 동일하지만 자유도에 따라 보정된 값  
   
- R-squared : 0에서 1까지 모델에 의해 설명된 분산의 비율 (결정계수 coefficient of determination, R2)  
   
- t 통계량 (t-statistic) : 계수의 표준오차로 나눈 예측변수의 계수. 모델에서 변수의 중요도를 비교하는 기준이 됨  
   
- 가중회귀 (weighted regression) : 서로 다른 가중치를 가진 레코드들을 회귀하는 방법  
   
<br>  

🎲 최소제곱법을 이용한 피팅, 적합값과 잔차 같은 단순선형회귀에서의 모든 개념은 다중선형회귀에도 그대로 적용됨  

$fitted = \hat{Y_{i}} = \hat{b_{0}} + \hat{b_{1}}X_{1,i} + \hat{b_{2}}X_{2,i} + ... + \hat{b_{p}}X_{p,i}$  

<br>  

***  

## 🎰 01. 모형평가  

🎲 제곱근평균제곱오차 (RMSE, root mean squared error) : 예측된 적합값들의 평균제곱오차의 제곱근을 의미함  

전반적인 모델의 정확도를 측정하고 다른 모델과 비교하기 위한 기준으로 사용됨  


$RMSE = \sqrt{\frac{\sum\nolimits_{i=1}^{n}{(y_i - \hat{y_i})^2}}{n}}$  

<br>  


🎲 잔차 표준오차 (RSE, residual standard error) : RMSE를 자유도에 따라 보정한 값  

$RSE = \sqrt{\frac{\sum\nolimits_{i=1}^{n}{(y_i - \hat{y_i})^2}}{n-p-1}}$  

<br>  

실제 선형회귀분석을 할 때, RMSE와 RSE의 차이는 아주 작음  

<br>  

🎲 R-squared (결정계수) : 0에서 1까지의 범위로 모델 데이터의 변동률을 측정하여 모델이 데이터에 얼마나 적합한지 평가함. 회귀분석을 설명하기 위한 용도로 활용.   


$R^2 = 1 - \frac{\sum\nolimits_{i=1}^{n}{(y_i - \hat{y_i})^2}}{\sum\nolimits_{i=1}^{n}{(y_i - \bar{y_i})^2}}$  

<br>  

🎲  adjusted R-squared : 모델에 독립변수가 추가되면 그 성능은 당연히 증가하기에, 더 많은 독립변수를 추가하는 것에 대해 페널티를 가해 결정계수를 산출함  

- 레코드 수 n, 모델의 변수 개수 P에 대해서  
 

$R_{adj}^2 = 1 - (1-R^2)\frac{n-1}{n-P-1}$

<br>  

🎲 t-통계량 : 계수가 통계적으로 유의미한 정도, 즉 독립변수와 종속변수를 랜덤하게 재배치했을 때 우연히 얻을 수 있는 범위를 어느 정도 벗어났는지를 측정  

계수의 표준오차 SE에 대해서, 

$t = \frac{\hat{b}}{SE(\hat{b})}$  

<br>  


t통계량이 높고 p-value가 낮을수록 독립변수는 더욱 유의미함  

즉 높은 t 통계량과 0에 가까운 p-value는 독립변수를 모델에 포함해야 된다는 의미이고, 낮은 t 통계량은 독립변수를 삭제할 수 있음을 나타냄  


불필요한 가정을 최소하하고 가장 단순하게 설명한다는 오컴의 면도날 원리에 의해, 이 두 값을 사용해 변수를 선택하는 것은 큰 도움이 됨  


🎲 모델을 평가하는 가장 중요한 지표는 제곱근평균제곱오차(RMSE)와 R-squared 임  

🎲 계수들의 표준오차는 모델에 대한 변수 기여도의 신뢰도를 측정하는 데 사용됨  



<br>  

***  

## 🎰 02. 교차타당성검사  

🎲 R-squared, F-통계량, t-통계량, p-value는 모두 표본 내에서 산출한 지표임.   

🎲 즉, 모델을 구하는 데 사용했던 데이터를 똑같이 그대로 사용하기 때문에 모델의 성능 확인에 오류가 있을 수 있음  

🎲 따라서 원래 데이터의 일부를 떼어놓고, 모델을 만든 후 떼어놓았던(홀드아웃) 데이터를 모델에 적용하면 모델의 성능 확인이 가능함  

🎲 일반적으로 데이터의 다수는 적합한 모델을 찾는 데 사용하고, 소수는 모델을 테스트하는 데 사용  

🎲 하지만 홀드아웃 샘플을 사용한다 하더라도, 상대적으로 작은 홀드아웃 샘플의 변동성으로 인해 불확실성을 초래할 수 있음  

<br>  

🎲 따라서 하나의 홀드아웃 샘플을 사용하지 않고 여러 개의 홀드아웃 샘플을 사용해 모델의 성능을 확인함  

🎲 이를 교차타당성검사 (cross-validation) 및 k 다중 교차타당성검사 (k-fold cross-validation) 이라고 함  

<br>  

🎲 알고리즘  

1. 1/k의 데이터를 홀드아웃 샘플로 따로 떼어놓음  
2. 남아 있는 데이터로 모델을 훈련시킴  
3. 모델을 1/k 홀드아웃 샘플에 적용하고 필요한 모델 평가 지표를 기록함  
4. 데이터의 첫 번째 1/k을 복원하고 앞서 선택했던 레코드는 제외한 다음 1/k을 따로 보관해둠  
5. 2~3단계를 반복  
6. 모든 레코드가 홀드아웃 샘플로 사용될 때까지 반복  
7. 모델 평가 지표들을 평균과 같은 방식으로 하나의 수치로 결합  
   
<br>  

***  

## 🎰 03. 모형 선택 및 단계적 회귀  

회귀분석 문제에서는 많은 변수를 독립변수로 사용할 수 있음  

하지만 많은 변수를 추가한다고 해서 항상 더 좋은 모델을 얻는 것은 아님  

-> 오컴의 면도날 원리 : 모든 것이 동일한 조건에서는 복잡한 모델보다 단순한 모델을 사용해야 함  

변수를 추가할수록 학습 데이터에 대해 항상 RMSE는 감소하고 R-squared는 증가 -> 추가하는 변수들은 모델 선택에 별로 도움이 되지 않음  

따라서 모델의 복잡성을 고려하기 위한 방법으로 adjusted-R-squared를 사용함  

또 다른 방법으로는 모델에 항을 추가할수록 불이익을 주는 AIC (Akaike’s information criteria)라는 측정 기준을 사용함  

<br>  

회귀분석에서 레코드 수 n, 모델의 변수 개수 P에 대해 

$AIC = 2P + n\log(RSS/n)$  

목표는 AIC를 최소화하는 모델을 찾는 것 -> 만약 모델에 k개의 변수를 추가한다면 2k만큼의 불이익을 받게 됨  

<br>  

결국 AIC를 최소로하거나 수정 R 제곱을 최대로 하는 모델을 찾아야 하는데, 이에 대한 방법으로 부분집합회귀로 모든 가능한 모델을 검색하는 방법이 있음  

-> 하지만 연산이 복잡하고 빅데이터나 변수가 많은 경우 적합하지 않음  

따라서 대안으로 단계적 회귀 (stepwise regression)를 사용함 -> 모델을 만드는 데 필요한 변수들을 자동으로 결정하는 방법  


- backward elimination : 전체 모델부터 시작하여 별로 의미 없는 변수들을 연속적으로 삭제  
   
- Forward Selection : 상수 모델에서 시작하여 연속적으로 R-squared에 가장 큰 기여도를 갖는 변수를 추가  
   
- backward selection : 전체 모델로 시작해 모든 독립변수가 통계적으로 유의미한 모델이 될 때까지 유의하지 않은 독립변수를 제거  
   
   
- stepwise regression : 독립변수를 연속적으로 추가 / 삭제하여 AIC를 낮추거나 R-squared를 높이는 모델을 찾음  
   
- penalized regression : 모델의 회귀식에 많은 변수에 대한 불이익을 주는 제약 조건을 추가  
   
   - stepwise regression 처럼 독립변수를 완전히 제거하는 대신 계수 크기를 감소시키거나 경우에 따라 거의 0으로 만들어 벌점을 적용함  
      
   - ex) 능형회귀, 라소  
      
<br>  

하지만 앞선 절에서 언급했듯이 단계적 회귀분석은 표본 내에서 모델을 평가하고 조정하는 방법임. 이에 선택된 모델이 오버피팅될 수 있으며, 새 데이터를 적용할 때 잘 맞지 않을 수 있음. 이를 방지하기 위한 접근법 중 하나는 교차타당성검사를 통해 모델의 유효성을 알아보는 것. 다만 선형회귀분석에서는 모델이 단순한 선형 구조와 전역 데이터 구조를 갖고 있기에 일반적으로 과적합 문제가 크게 나타나지 않음. 따라서 복잡한 모델 유형과 좁은 영역의 데이터 구조에 반응하는 반복적인 절차의 경우 교차타당성검사는 매우 중요함  

<br>  

***  

## 🎰 04. 가중회귀  

🎲 방정식을 피팅할 때 레코드별로 가중치를 주기 위해 사용  


🎲 가중회귀의 유용성은 아래와 같음  

- 서로 다른 관측치를 다른 정밀도로 측정했을 때, 역분산 가중치를 얻을 수 있음. 분산이 높을수록 가중치가 낮음  
   
- 가중치 변수가 데이터의 각 행이 나타내는 원본 관측치 수를 반영하도록 행이 여러 경우를 의미하는 데이터를 분석할 수 있음  
 
<br>  

연도의 경우 오래된 정보일수록 최근 정보보다 신뢰하기 어려움. 따라서 데이터 수집을 시작한 후 지난 년수를 가중치로 사용할 수도 있음  

<br>  

***  


