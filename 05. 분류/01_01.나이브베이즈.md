## 🎰 분류(classification)  

결과를 알고 있는 데이터를 이용해 모델을 학습시켜 결과가 알려지지 않은 데이터에 모델을 적용하는 지도 학습  

- 데이터가 1인지 0인지 분류  
   
- 여러 카테고리 중 어디에 속할지 예측  
   
- 범주의 개수가 두 가지 이상인 분류 -> 각 클래스에 속할 예측 확률 파악  
 
<br>  

🎲 대부분의 알고리즘은 관심 클래스에 속할 확률을 반환 -> 컷오프 확률(기준확률)을 통해 확률로부터 결정을 내림  

1. 어떤 레코드가 속할 거라고 생각되는 관심 클래스에 대한 컷오프 확률을 정함  
2. 레코드가 관심 클래스에 속할 확률을 추정  
3. 그 확률이 컷오프 확률 이상이면 관심 클래스에 해당 레코드를 할당  
   
🎲 컷오프가 높을수록 1로 예측되는 관심 클래스에 속할 레코드가 적어짐.  

🎲 컷오프가 낮을수록 더 많은 레코드가 1로 예측됨  

<br>  

🎲 범주형 클래스가 두 가지 이상인 경우에는 0,1,2로 클래스를 할당하여 분류를 진행함  

1. Y = 0 인지 Y > 0 인지 예측  
2. Y > 0 인 경우, Y = 1 인지 Y = 2 인지 예측  
   
<br>  

***  
<br>  

# 🎰 01_01. 나이브 베이즈  

🎲 독립변수와 종속변수 모두 범주형 변수인 경우 사용 가능  

🎲 클래스에 대해서, 각 클래스가 나올 확률이 범주형 독립변수의 어떤 카테고리에서 나올 확률이 가장 높은지 구하는 과정  

🎲 위의 단계를 통해 주어진 독립변수에 대해서 결과 클래스의 결과를 추정하는 것으로 변환됨  

<br>  

🎲 주어진 결과에 대해 독립변수값을 관찰할 확률을 사용하여, 종속변수가 주어졌을 때 결과 Y = i를 관찰할 확률을 추정  

🎲 즉, 정말 관심 있는 것을 추정  

<br>  

🎲 용어 정리  

- 조건부확률 (conditional probability) : 어떤 사건(Y = i)이 주어졌을 때, 해당 사건(X = i)을 관찰할 확률. $P(X_i | Y_i)$  
   
- 사후확률 (posterior probability) : 예측 정보를 통합한 후 결과의 확률  
   
<br>  

🎲 정확한 베이즈 분류기 ( <-> 나이브 베이즈)  

1. 예측변수의 값이 동일한 모든 레코드들을 확인  
2. 해당 레코드들이 가장 많이 속한 독립변수의 카테고리를 정의  
3. 새 레코드를 확인하고, 유사한 경우 해당 클래스를 지정  
   
즉 모든 독립변수들이 동일하면 같은 클래스일 가능성이 높기에, 새 레코드와 정확히 일치하는 표본 내의 데이터를 찾는 것을 사용하는 방식  

<br>  

***  
<br>  


## 🎰 01. 나이브하지 않은 베이즈 분류가 현실성이 없는 이유  

정확한 베이즈 분류기는 모든 독립변수가 동일할 가능성을 염두에 두고 각 가능성을 따짐  

하지만 예측변수의 개수가 일정 정도 커지게 되면, 분류해야 하는 데이터들은 대부분은 서로 완전 일치하는 경우가 거의 없음  

따라서 정확한 베이즈 분류기를 현실에서 사용하는 것은 거의 불가능함  

<br>  

***  

<br>  

## 🎰 02. 나이브한 해법  

정확한 베이즈 해법은 정확히 일치하는 레코드만을 사용해 확률을 계산함  

하지만 나이브 베이즈는 전체 데이터를 활용함  

<br>  

🎲 과정  

1. 이진 응답 $Y = i$ ($i = 0$ 또는 1)에 대해, 각 독립변수에 대한 조건부확률 $P(X_j | Y = i)$를 구함  
2. 1에서 구한 값은 $Y = i$ 가 주어질 때 독립변수의 값이 나올 확률이며, training set에서 $Y = i$ 인 레코드들 중 $X_j$ 값의 비율로 구할 수 있음  
3. 각 조건부확률을 곱한 다음, $Y = i$에 속한 레코드들의 비율을 곱함  
4. 모든 클래스에 대해 1~3 단계를 반복  
5. 3단계에서 모든 클래스에 대해 구한 확률값을 모두 더한 값으로 클래스 $i$의 확률을 나누면 $i$ 가 나올 확률을 구할 수 있음  
6. 각 독립변수에 대해 가장 높은 확률을 갖는 클래스를 해당 레코드에 할당  
 
<br>  

🎲 나이브 베이즈 알고리즘은 독립변수 $X_1, … , X_p$ 가 주어졌을 때의 출력 $Y = i$의 확률에 대한 방정식으로 표현될 수 있음  

$P(Y=i\\;|\\;X_1,\\;X_2,\\;...,\\;X_p) \\; \\; \\; (단, \\; \\; i \\; = \\; 0 \\; or \\; 1)$  

<br>  

🎲 베이즈 분류를 사용하여 클래스 확률을 계산하기 위한 공식  

$P(Y=i\\;|\\;X_1,\\;X_2,\\;...,\\;X_p)\\;=\\;\frac{P(Y=i)P(X_1,\\;...,\\;X_p\\;|\\;Y=i)}{P(Y=0)P(X_1,\\;...,\\;X_p\\;|\\;Y=0)\\;+\\;P(Y=1)P(X_1,\\;...,\\;X_p\\;|\\;Y=1)}$  

<br>  

🎲 $k ≠ j$ 인 모든 $X_j$ 가 $X_k$ 와 독립이라는 나이브 베이즈 가정 하에, 위 방정식을 전개하면  

$P(Y=i\\;|\\;X_1,\\;X_2,\\;...,\\;X_p)\\;=\\;\frac{P(Y=i)P(X_1\\;|\\;Y=i)\\;...\\;P(X_p\\;|\\;Y=i)}{P(Y=0)P(X_1\\;|\\;Y=0)\\;...\\;P(X_p\\;|\\;Y=0)\\;+\\;P(Y=1)P(X_1\\;|\\;Y=1)\\;...\\;P(X_p\\;|\\;Y=1)}$  

<br>  

🎲 다만, 나이브 베이즈 분류기는 편향된 추정 결과를 예측하는 것으로 잘 알려져 있음  

🎲 하지만 Y = 1 인 확률에 따라 레코드들에 순위를 매기는 것이 목적일 때는 우수한 성능을 보임  

<br>  

***  

<br>  

## 🎰 03. 수치형 독립변수  

베이즈 분류기는 예측변수들이 범주형인 경우(스팸 메일 분류에서 특정 단어, 어구, 문자열의 존재 여부 등)에 적합함  

만약 수치형 변수에 나이브 베이즈 방법을 적용하려면, 두 가지 방법이 존재함  

- 수치형 예측변수를 비닝 (binning) 하여 범주형으로 변환한 뒤, 알고리즘을 적용  
   
- 조건부확률 추정을 위해 정규분포와 같은 확률모형 사용  
   
<br>  

만약 훈련 데이터에 독립변수의 특정 카테고리에 해당하는 데이터가 없으면 새 데이터 결과에 대한 확률을 0으로 할당함  

<br>  

***  

