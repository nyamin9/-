## 🎰 분류(classification)  

결과를 알고 있는 데이터를 이용해 모델을 학습시켜 결과가 알려지지 않은 데이터에 모델을 적용하는 지도 학습  

- 데이터가 1인지 0인지 분류  
   
- 여러 카테고리 중 어디에 속할지 예측  
   
- 범주의 개수가 두 가지 이상인 분류 -> 각 클래스에 속할 예측 확률 파악  
 
<br>  

🎲 대부분의 알고리즘은 관심 클래스에 속할 확률을 반환 -> 컷오프 확률(기준확률)을 통해 확률로부터 결정을 내림  

1. 어떤 레코드가 속할 거라고 생각되는 관심 클래스에 대한 컷오프 확률을 정함  
2. 레코드가 관심 클래스에 속할 확률을 추정  
3. 그 확률이 컷오프 확률 이상이면 관심 클래스에 해당 레코드를 할당  
   
🎲 컷오프가 높을수록 1로 예측되는 관심 클래스에 속할 레코드가 적어짐.  

🎲 컷오프가 낮을수록 더 많은 레코드가 1로 예측됨  

<br>  

🎲 범주형 클래스가 두 가지 이상인 경우에는 0,1,2로 클래스를 할당하여 분류를 진행함  

1. Y = 0 인지 Y > 0 인지 예측  
2. Y > 0 인 경우, Y = 1 인지 Y = 2 인지 예측  
   
<br>  

***  
<br>  

# 🎰 01_01. 나이브 베이즈  

🎲 독립변수와 종속변수 모두 범주형 변수인 경우 사용 가능  

🎲 클래스에 대해서, 각 클래스가 나올 확률이 범주형 독립변수의 어떤 카테고리에서 나올 확률이 가장 높은지 구하는 과정  

🎲 위의 단계를 통해 주어진 독립변수에 대해서 결과 클래스의 결과를 추정하는 것으로 변환됨  

<br>  

🎲 주어진 결과에 대해 독립변수값을 관찰할 확률을 사용하여, 종속변수가 주어졌을 때 결과 Y = i를 관찰할 확률을 추정  

🎲 즉, 정말 관심 있는 것을 추정  

<br>  

🎲 용어 정리  

- 조건부확률 (conditional probability) : 어떤 사건(Y = i)이 주어졌을 때, 해당 사건(X = i)을 관찰할 확률. $P(X_i | Y_i)$  
   
- 사후확률 (posterior probability) : 예측 정보를 통합한 후 결과의 확률  
   
<br>  

🎲 정확한 베이즈 분류기 ( <-> 나이브 베이즈)  

1. 예측변수의 값이 동일한 모든 레코드들을 확인  
2. 해당 레코드들이 가장 많이 속한 독립변수의 카테고리를 정의  
3. 새 레코드를 확인하고, 유사한 경우 해당 클래스를 지정  
   
즉 모든 독립변수들이 동일하면 같은 클래스일 가능성이 높기에, 새 레코드와 정확히 일치하는 표본 내의 데이터를 찾는 것을 사용하는 방식  

<br>  

***  
<br>  


## 🎰 01. 나이브하지 않은 베이즈 분류가 현실성이 없는 이유  

정확한 베이즈 분류기는 모든 독립변수가 동일할 가능성을 염두에 두고 각 가능성을 따짐  

하지만 예측변수의 개수가 일정 정도 커지게 되면, 분류해야 하는 데이터들은 대부분은 서로 완전 일치하는 경우가 거의 없음  

따라서 정확한 베이즈 분류기를 현실에서 사용하는 것은 거의 불가능함  

<br>  

***  


